{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4o0uCysJdGN"
      },
      "source": [
        "# Toxicity Detection Model\n",
        "\n",
        "Get live data with `get_comments.py` and transform Kaggle data set with `transform_train.py`\n",
        "\n",
        "Get libraries by running `pip install -r requirements.txt`\n",
        "\n",
        "This is a deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "id": "EAifQtSGJq7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eo3EBLEiJdGX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, Model\n",
        "from keras.layers import Input, Bidirectional, Dense, Embedding, LSTM, Dropout\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from wordcloud import STOPWORDS, wordcloud\n",
        "import re\n",
        "from fuzzywuzzy import fuzz, process\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "from tqdm import tqdm\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk6V5JKTJdGc"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1MtOJQuJqiOHUKS8SvgQAfuTamwI5nagp"
      ],
      "metadata": {
        "id": "mQ4WYrOdJnkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7gIw83yJdGd"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('new_train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGmkZ2RqJdGe"
      },
      "source": [
        "### Data Cleansing\n",
        "<ul>\n",
        "    <li>Removal of special characters</li>\n",
        "    <li>Expanding contractions</li>\n",
        "    <li>Removal of stopping words</li>\n",
        "    <li>Lowering text</li>\n",
        "    <li>Replacing Obfuscated Profane Words</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6vwyaSdJdGg"
      },
      "outputs": [],
      "source": [
        "def decontracted(phrase):\n",
        "    '''\n",
        "    This function decontracts words like won't to will not\n",
        "    '''\n",
        "\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    \n",
        "    return phrase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4cLuLE8JdGi"
      },
      "outputs": [],
      "source": [
        "def removeNonPrintable(com):\n",
        "    com = com.replace('\\\\r', ' ')\n",
        "    com = com.replace('\\\\n', ' ')\n",
        "    com = com.replace('\\\\t', ' ')\n",
        "    com = com.replace('\\\\\"', ' ')\n",
        "    return com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbRlbk1VJdGk"
      },
      "outputs": [],
      "source": [
        "def getUniqueWords(comments):\n",
        "    unique_words = set()\n",
        "    for comment in tqdm(comments):\n",
        "        words = comment.split(\" \")\n",
        "        for word in words:\n",
        "            if len(word) > 2:\n",
        "                unique_words.add(word)\n",
        "    \n",
        "    return unique_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcQrMnfsJdGl"
      },
      "outputs": [],
      "source": [
        "def getProfaneWords():\n",
        "    profane_words = []\n",
        "    with open(\"bad-words.txt\",\"r\") as f:\n",
        "        for word in f:\n",
        "            word = word.replace(\"\\n\",\"\")\n",
        "            profane_words.append(word)\n",
        "    return profane_words\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7WJNXzXJdGm"
      },
      "outputs": [],
      "source": [
        "def createMappingDict(profane_words, unique_words):\n",
        "    # mapping dictionary\n",
        "    mapping_dict = dict()\n",
        "    \n",
        "    # looping through each profane word\n",
        "    for profane in tqdm(profane_words):\n",
        "        mapped_words = set()\n",
        "        \n",
        "        # looping through each word in vocab\n",
        "        for word in unique_words:\n",
        "            # mapping only if ratio > 80\n",
        "            try:\n",
        "                if fuzz.ratio(profane,word) > 80:\n",
        "                    mapped_words.add(word)\n",
        "            except:\n",
        "                pass\n",
        "                \n",
        "        # list of all vocab words for given profane word\n",
        "        mapping_dict[profane] = mapped_words\n",
        "    \n",
        "    return mapping_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxu-qzVrJdGn"
      },
      "outputs": [],
      "source": [
        "def replaceWords(corpus, mapping_dict):\n",
        "    processed_corpus = []\n",
        "\n",
        "    for document in tqdm(corpus):\n",
        "\n",
        "        # words = document.split()\n",
        "\n",
        "        for mapped_word, v in mapping_dict.items():\n",
        "            \n",
        "            document = re.sub(r'\\b{word}\\b'.format(word = v), mapped_word, document)\n",
        "\n",
        "            # for target_word in v:\n",
        "\n",
        "            #     for i, word in enumerate(words):\n",
        "            #         if word == target_word:\n",
        "            #             words[i] = mapped_word\n",
        "\n",
        "        # document = \" \".join(words)\n",
        "        document = document.strip()\n",
        "\n",
        "        processed_corpus.append(document)\n",
        "\n",
        "    return processed_corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-ZFhK35JdGp"
      },
      "outputs": [],
      "source": [
        "def final_processing(corpus):\n",
        "    '''\n",
        "    Function applies final processing steps post profane mapping such as removing special characters,\n",
        "    punctuations etc.\n",
        "    '''\n",
        "    processed_comments = []\n",
        "    stopwords_list = stopwords.words(\"english\")\n",
        "    # stopwords_list = nltk.download('stopwords')\n",
        "    print('final_processing')\n",
        "\n",
        "    emoj = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "\n",
        "\n",
        "    # looping through each comment in corpus\n",
        "    for comment in tqdm(corpus):\n",
        "        try:\n",
        "          processed_words = []\n",
        "          comment = re.sub(emoj, r'', comment)\n",
        "          comment = re.sub(r'http\\S+', '', comment)\n",
        "          # comment = re.sub(\"\\B\\#\\w+\", ' ', comment)\n",
        "          # comment = re.sub(\"\\B\\@\\w+\", '', comment)\n",
        "          comment = re.sub(r'(\\w*#\\w+|\\w+#\\w*)','', comment)\n",
        "          comment = re.sub(r'(\\w*@\\w+|\\w+#\\w*)','',  comment)\n",
        "          comment = re.sub(r'[^A-Za-z\\s]+',\"\",  comment) # retain only letters\n",
        "          for word in comment.split():\n",
        "              if len(word) >= 3:\n",
        "                  processed_words.append(word)\n",
        "          \n",
        "          comment = \" \".join([e for e in processed_words if e.lower() not in stopwords_list])\n",
        "          processed_comments.append(comment.strip())\n",
        "        except Exception as e:\n",
        "          print(corpus)\n",
        "          print(e)\n",
        "          pass\n",
        "    \n",
        "    return processed_comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4GPWLgSJdGq"
      },
      "outputs": [],
      "source": [
        "def cleanComments(comments):\n",
        "    processed_comments = []\n",
        "    for comment in comments:\n",
        "        comment = decontracted(comment)\n",
        "        comment = removeNonPrintable(comment)\n",
        "\n",
        "        # Lower comment\n",
        "        processed_comments.append(comment.lower().strip())\n",
        "      \n",
        "    profane_words = getProfaneWords()\n",
        "    unique_words = getUniqueWords(processed_comments)\n",
        "    profane_dict = createMappingDict(profane_words, unique_words)\n",
        "    processed_comments = replaceWords(processed_comments, profane_dict)\n",
        "    final_comments = final_processing(processed_comments)\n",
        "    return final_comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPPd7Sg7JdGr"
      },
      "outputs": [],
      "source": [
        "def corrHeatmap(df, fileName = None):\n",
        "    classes = df.columns[1:]\n",
        "    data = df.copy()\n",
        "    data = data[classes]\n",
        "    corr = data.corr()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(corr,annot=True,vmin=1,vmax=0,fmt='.2g',cmap='rocket')\n",
        "    plt.title(\"Correlation Matrix: Labels of Comments\")\n",
        "    if fileName:\n",
        "        plt.savefig(fileName + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrl6vNPlJdGr"
      },
      "outputs": [],
      "source": [
        "def showPercentiles(data):\n",
        "    print(\"========== For 0-100 ==========\")\n",
        "    for i in range(11):\n",
        "        print(f'{i*10}th Percentile Value = {np.percentile(data, i*10)}')\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"========== For 90-100 ==========\")\n",
        "    for i in range(11):\n",
        "        print(f'{90+i}th Percentile Value = {np.percentile(data, 90 + i)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UalYBmPaJdGs"
      },
      "outputs": [],
      "source": [
        "def plotHist(data, bin_size, title, fileName = None, columns = None):\n",
        "    if columns is not None:\n",
        "        fig, axis = plt.subplots(len(columns)-1, 1, figsize = (12,30))\n",
        "        for i, col in enumerate(columns[1:]):\n",
        "            sns.histplot(data=data[col],bins=bin_size,palette=\"rocket\",ax = axis[i])\n",
        "\n",
        "        fig.tight_layout()\n",
        "        fig.subplots_adjust(top = 0.95)\n",
        "        fig.suptitle(title, size = 18)\n",
        "    else:\n",
        "        sns.histplot(data=data,bins=bin_size,palette=\"rocket\")\n",
        "        plt.title(title)\n",
        "    \n",
        "\n",
        "    if fileName:\n",
        "        plt.savefig(fileName + '.png')\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL4gF8OHJdGt"
      },
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daSu_QehJdGv"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T76PbuSHJdGv"
      },
      "outputs": [],
      "source": [
        "plotHist(df_train, 30, 'Histogram of the Labels', 'categories_histogram', df_train.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3PPZ9ynJdGw"
      },
      "outputs": [],
      "source": [
        "plotHist(df_train.Comment.str.len(), 50, 'Length of the Comments', 'comments_length_before_preprocess')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4PSb6vNJdGy"
      },
      "outputs": [],
      "source": [
        "for cat in df_train.columns[1:]:\n",
        "    print(f\"============== {cat} ==============\")\n",
        "    showPercentiles(df_train[cat])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkzxiTNXJdGz"
      },
      "outputs": [],
      "source": [
        "showPercentiles(df_train.Comment.str.len())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz5_84BDJdGz"
      },
      "outputs": [],
      "source": [
        "corrHeatmap(df_train, 'corr_matrix_labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnX2z1DjJdG0"
      },
      "source": [
        "There is a strong positive relationship between Toxicity-Severe Toxicity, Identity Attack-Insult and Toxicity-Profanity respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqznn-8SJdG0"
      },
      "outputs": [],
      "source": [
        "processed_comments = cleanComments(df_train.Comment.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6RMrFrzJdG1"
      },
      "outputs": [],
      "source": [
        "df_processed = df_train[df_train.columns[1:]].copy()\n",
        "df_processed['ProcessedComment'] = processed_comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfasVWgyJdG2"
      },
      "outputs": [],
      "source": [
        "cols = df_processed.columns.tolist()\n",
        "ordered_cols = cols[-1:] + cols[:-1]\n",
        "df_processed = df_processed[ordered_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1sJ2FwBJdG3"
      },
      "outputs": [],
      "source": [
        "df_processed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_28lUwe9JdG3"
      },
      "outputs": [],
      "source": [
        "df_processed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWalIW_ZJdG3"
      },
      "outputs": [],
      "source": [
        "plotHist(df_processed.ProcessedComment.str.len(), 50, 'Comments Length After Preprocessing', 'after_preprocess_comments_histogram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8y5eQR_JdG4"
      },
      "source": [
        "As seen from the histogram above, the length most of the comments have is in between 0-300. Additionally, the histogram fits to logarithmic distribution and it is right-skewed. To examine the histogram furher, we can look into percentiles of the histogram to determine the vector dimensions which will be crucial for our deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NopHSyg-JdG4"
      },
      "outputs": [],
      "source": [
        "showPercentiles(df_processed.ProcessedComment.str.len())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDu_jILDJdG5"
      },
      "source": [
        "Almost 95% of the comments have 1000 characters approximately. As a result, the input dimension of the tokenized vectors can be 1000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9idEfZbJdG5"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Ov5caJJdG6"
      },
      "source": [
        "Turn dataset into list(list of tokens, scores x6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lieKcmEzJdG6"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE+1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_list = df_processed.dropna().loc[:,\"Toxicity\": \"Threat\"].convert_dtypes(infer_objects=True).__array__()[:,np.newaxis,:]"
      ],
      "metadata": {
        "id": "Xu_n-2F7MdDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed.head()"
      ],
      "metadata": {
        "id": "qLdKShW9ML4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_processed['ProcessedComment'].values, labels_list, train_size = 0.9)"
      ],
      "metadata": {
        "id": "T8Z-A8BCL9Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train[:,0,:]\n",
        "y_test = y_test[:,0,:]\n"
      ],
      "metadata": {
        "id": "pJCdccNTNYnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "q5Zgvg39NyKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(sequences=X_train, maxlen=VOCAB_SIZE, padding='post', truncating='post')\n",
        "X_test = pad_sequences(sequences=X_test, maxlen=VOCAB_SIZE, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "NxMKKsuON_ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train).astype(np.float32)\n",
        "X_test = np.asarray(X_test).astype(np.float32)\n",
        "y_train = np.asarray(y_train).astype(np.float32)\n",
        "y_test = np.asarray(y_test).astype(np.float32)"
      ],
      "metadata": {
        "id": "w7--BCZtQObM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "1946XQgWQ2H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# saving the training, validation and test tokenized data\n",
        "fp = \"tokenized_data.pkl\"\n",
        "with open(fp,mode=\"wb\") as f:\n",
        "    pickle.dump(obj=(X_train,\n",
        "                     y_train,\n",
        "                     X_test,\n",
        "                     y_test),\n",
        "                file=f)"
      ],
      "metadata": {
        "id": "HjRkOs9GOYfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequential_architecture():\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(layers.Dense(64, input_dim=VOCAB_SIZE, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(layers.Dense(32, kernel_initializer='he_uniform'))\n",
        "  model.add(layers.Dense(6))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "TRtGxhJ2MK7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOh56ra2JdG7"
      },
      "outputs": [],
      "source": [
        "model_seq = get_sequential_architecture()\n",
        "model_seq.compile(loss = keras.losses.BinaryCrossentropy(from_logits = True),\n",
        "          optimizer = keras.optimizers.adam_v2.Adam(1e-4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq.fit(x=X_train,\n",
        "\t\t\t\t\ty=y_train,\n",
        "\t\t\t\t\tepochs = 100,\n",
        "\t\t\t\t\tsteps_per_epoch = math.floor(len(X_train)/100),\n",
        "\t\t\t\t\tverbose=1,\n",
        "\t\t\t\t\tvalidation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "f4HOpYXENohp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lstm_architecture(max_length,vocab_size,embedding_matrix):\n",
        "    '''\n",
        "    Function creates LSTM architecture with the input embedding matrix specified \n",
        "    '''\n",
        "\n",
        "    # clearing backend session\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # defining input and embedding layers\n",
        "    input_layer = Input(shape=(max_length,))\n",
        "    # embedding = Embedding(input_dim=vocab_size,output_dim=300,input_length=max_length,weights=[embedding_matrix],trainable=False)(input_layer) \n",
        "    embedding = Embedding(input_dim=vocab_size,output_dim=300,input_length=max_length, trainable=True)(input_layer) \n",
        "\n",
        "    # bi-directional lstm layers\n",
        "    lstm_output_1 = Bidirectional(LSTM(units=64,return_sequences=True))(embedding)\n",
        "    drop = Dropout(rate=0.5)(lstm_output_1)\n",
        "    lstm_output_2 = Bidirectional(LSTM(units=64,return_sequences=False))(drop)\n",
        "\n",
        "    # output layer\n",
        "    output_layer = Dense(units=6,activation='sigmoid')(lstm_output_2)\n",
        "\n",
        "    # creating the model\n",
        "    model = Model(inputs=input_layer,outputs=output_layer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "t62jEPwzK38t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = get_lstm_architecture(VOCAB_SIZE, VOCAB_SIZE+1, None)"
      ],
      "metadata": {
        "id": "zKtKRo75PMuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_lstm.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy', metrics = ['MAE'])"
      ],
      "metadata": {
        "id": "2W3muMb1Pwsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                               patience=1,\n",
        "                               verbose=1)\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\",\n",
        "                           patience=2,\n",
        "                           verbose=1)"
      ],
      "metadata": {
        "id": "8QK1u6BpuytK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(X_train, y_train, batch_size = 32, epochs = 10, validation_data = (X_test[:-5000], y_test[:-5000]))"
      ],
      "metadata": {
        "id": "X_LTjD9iPnai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.save(\"./model_lstm\")"
      ],
      "metadata": {
        "id": "Wt6FsTnN3NIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_lstm.predict(X_test[-5000:])"
      ],
      "metadata": {
        "id": "PdhSZgZr3b42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK90JmP_JdG8"
      },
      "outputs": [],
      "source": [
        "model_lstm.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXOVImIFJdG8"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(x=test_input, y=test_output)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "9c27eb6091674c73af7ac585eb40475ca82f7aaec6bf96c7e19d14c7a47e78dc"
    },
    "kernelspec": {
      "display_name": "Python 3.6.6 ('python-cvcourse')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "AI Project Notebook",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}