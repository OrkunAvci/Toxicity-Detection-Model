{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Detection Model\n",
    "\n",
    "Get live data with `get_comments.py` and transform Kaggle data set with `transform_train.py`\n",
    "\n",
    "Get libraries by running `pip install -r requirements.txt`\n",
    "\n",
    "This is a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import STOPWORDS, wordcloud\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/new_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "<ul>\n",
    "    <li>Removal of special characters</li>\n",
    "    <li>Expanding contractions</li>\n",
    "    <li>Lowering text</li>\n",
    "    <li>Replacing Obfuscated Profane Words</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    '''\n",
    "    This function decontracts words like won't to will not\n",
    "    '''\n",
    "\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonPrintable(com):\n",
    "    com = com.replace('\\\\r', ' ')\n",
    "    com = com.replace('\\\\n', ' ')\n",
    "    com = com.replace('\\\\t', ' ')\n",
    "    com = com.replace('\\\\\"', ' ')\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWords(comments):\n",
    "    unique_words = set()\n",
    "    for comment in tqdm(comments):\n",
    "        words = comment.split(\" \")\n",
    "        for word in words:\n",
    "            if len(word) > 2:\n",
    "                unique_words.add(word)\n",
    "    \n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProfaneWords():\n",
    "    profane_words = []\n",
    "    with open(\"./data/bad-words.txt\",\"r\") as f:\n",
    "        for word in f:\n",
    "            word = word.replace(\"\\n\",\"\")\n",
    "            profane_words.append(word)\n",
    "    return profane_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMappingDict(profane_words, unique_words):\n",
    "    # mapping dictionary\n",
    "    mapping_dict = dict()\n",
    "    \n",
    "    # looping through each profane word\n",
    "    for profane in tqdm(profane_words):\n",
    "        mapped_words = set()\n",
    "        \n",
    "        # looping through each word in vocab\n",
    "        for word in unique_words:\n",
    "            # mapping only if ratio > 80\n",
    "            try:\n",
    "                if fuzz.ratio(profane,word) > 80:\n",
    "                    mapped_words.add(word)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # list of all vocab words for given profane word\n",
    "        mapping_dict[profane] = mapped_words\n",
    "    \n",
    "    return mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWords(corpus, mapping_dict):\n",
    "    processed_corpus = []\n",
    "\n",
    "    for document in tqdm(corpus):\n",
    "\n",
    "        # words = document.split()\n",
    "\n",
    "        for mapped_word, v in mapping_dict.items():\n",
    "            \n",
    "            document = re.sub(r'\\{word}\\b'.format(word = v), mapped_word, document)\n",
    "\n",
    "            # for target_word in v:\n",
    "\n",
    "            #     for i, word in enumerate(words):\n",
    "            #         if word == target_word:\n",
    "            #             words[i] = mapped_word\n",
    "\n",
    "        # document = \" \".join(words)\n",
    "        document = document.strip()\n",
    "\n",
    "        processed_corpus.append(document)\n",
    "\n",
    "    return processed_corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_processing(corpus):\n",
    "    '''\n",
    "    Function applies final processing steps post profane mapping such as removing special characters,\n",
    "    punctuations etc.\n",
    "    '''\n",
    "    processed_comments = []\n",
    "    processed_words = []\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "    \"]+\", re.UNICODE)\n",
    "\n",
    "\n",
    "    # looping through each comment in corpus\n",
    "    for comment in tqdm(corpus):\n",
    "        comment = re.sub(emoj, '', comment)\n",
    "        comment = re.sub(r'http\\S+', '', comment)\n",
    "        comment = re.sub(\"\\B\\#\\w+\", '', comment)\n",
    "        comment = re.sub(\"\\B\\@\\w+\", '', comment)\n",
    "        comment = re.sub('[^A-Za-z\\s]+',\"\", comment) # retain only letters\n",
    "        for word in comment.split():\n",
    "            if len(word) >= 3:\n",
    "                processed_words.append(word)\n",
    "            \n",
    "        comment = \" \".join(processed_words)\n",
    "        \n",
    "        processed_comments.append(comment.strip())\n",
    "    \n",
    "    return processed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanComments(comments):\n",
    "    processed_comments = []\n",
    "    for comment in comments:\n",
    "        comment = decontracted(comment)\n",
    "        comment = removeNonPrintable(comment)\n",
    "\n",
    "        # Lower comment\n",
    "        processed_comments.append(comment.lower().strip())\n",
    "    \n",
    "    profane_words = getProfaneWords()\n",
    "    unique_words = getUniqueWords(processed_comments)\n",
    "    profane_dict = createMappingDict(profane_words, unique_words)\n",
    "    processed_comments = replaceWords(comments, profane_dict)\n",
    "    return final_processing(processed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(len(df_train.columns)-1, 1, figsize = (12,30))\n",
    "for i, col in enumerate(df_train.columns[1:]):\n",
    "    counts, bins = np.histogram(df_train[col].values, bins = 30)\n",
    "    axis[i].hist(bins[:-1], bins, weights=counts)\n",
    "    axis[i].title.set_text(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_lens = df_train.Comment.str.len()\n",
    "counts, bins = np.histogram(comments_lens.values, bins = 50)\n",
    "plt.hist(bins[:-1], bins, weights=counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    print(f'{i*10}th Percentile Value = {np.percentile(comments_lens, i*10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    print(f'{90+i}th Percentile Value = {np.percentile(comments_lens, 90 + i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_comments = cleanComments(df_train.Comment.values[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn dataset into list(list of tokens, scores x6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = df_train.loc[:,\"Toxicity\": \"Threat\"].__array__()[:500]\n",
    "mapped = np.array([[processed_comments[i], detections[i]] for i in list(range(len(processed_comments)))], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mapped[: int(mapped.shape[0] * .9) ]\n",
    "test_dataset = mapped[int(mapped.shape[0] * .9) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = layers.TextVectorization(max_tokens = VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "\tencoder,\n",
    "\tlayers.Embedding(\n",
    "\t\tinput_dim = len(encoder.get_vocabulary()),\n",
    "\t\toutput_dim = 64,\n",
    "\t\t# Use masking to handle the variable sequence lengths\n",
    "\t\tmask_zero = True\n",
    "\t),\n",
    "\tlayers.Dropout(0.8),\n",
    "\tlayers.Bidirectional(layers.LSTM(64)),\n",
    "\tlayers.Dropout(0.6),\n",
    "\tlayers.Dense(32, activation='relu'),\n",
    "\tlayers.Dropout(0.5), # ?\n",
    "\tlayers.Dense(6)\n",
    "])\n",
    "\n",
    "model.compile(loss = keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "\t\t\t  optimizer = keras.optimizers.adam_v2.Adam(1e-4),\n",
    "\t\t\t  metrics = ['Toxicity', 'Severe_Toxicity', 'Identity_Attack', 'Insult', 'Profanity', 'Threat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "\t\t\t\t\tepochs = 20,\n",
    "\t\t\t\t\tsteps_per_epoch = 50,\n",
    "\t\t\t\t\tvalidation_data = test_dataset,\n",
    "\t\t\t\t\tvalidation_steps = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c92edb50b0d3c17cb874491556f3b23517a1eaeecdd98db06b8cc77892b6a96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
