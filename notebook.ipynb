{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity Detection Model\n",
    "\n",
    "Get live data with `get_comments.py` and transform Kaggle data set with `transform_train.py`\n",
    "\n",
    "Get libraries by running `pip install -r requirements.txt`\n",
    "\n",
    "This is a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import STOPWORDS, wordcloud\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/new_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing\n",
    "<ul>\n",
    "    <li>Removal of special characters</li>\n",
    "    <li>Expanding contractions</li>\n",
    "    <li>Lowering text</li>\n",
    "    <li>Replacing Obfuscated Profane Words</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    '''\n",
    "    This function decontracts words like won't to will not\n",
    "    '''\n",
    "\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonPrintable(com):\n",
    "    com = com.replace('\\\\r', ' ')\n",
    "    com = com.replace('\\\\n', ' ')\n",
    "    com = com.replace('\\\\t', ' ')\n",
    "    com = com.replace('\\\\\"', ' ')\n",
    "    return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniqueWords(comments):\n",
    "    unique_words = set()\n",
    "    for comment in tqdm(comments):\n",
    "        words = comment.split(\" \")\n",
    "        for word in words:\n",
    "            if len(word) > 2:\n",
    "                unique_words.add(word)\n",
    "    \n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProfaneWords():\n",
    "    profane_words = []\n",
    "    with open(\"./data/bad-words.txt\",\"r\") as f:\n",
    "        for word in f:\n",
    "            word = word.replace(\"\\n\",\"\")\n",
    "            profane_words.append(word)\n",
    "    return profane_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMappingDict(profane_words, unique_words):\n",
    "    # mapping dictionary\n",
    "    mapping_dict = dict()\n",
    "    \n",
    "    # looping through each profane word\n",
    "    for profane in tqdm(profane_words):\n",
    "        mapped_words = set()\n",
    "        \n",
    "        # looping through each word in vocab\n",
    "        for word in unique_words:\n",
    "            # mapping only if ratio > 80\n",
    "            try:\n",
    "                if fuzz.ratio(profane,word) > 80:\n",
    "                    mapped_words.add(word)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # list of all vocab words for given profane word\n",
    "        mapping_dict[profane] = mapped_words\n",
    "    \n",
    "    return mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWords(corpus, mapping_dict):\n",
    "    processed_corpus = []\n",
    "\n",
    "    for document in tqdm(corpus):\n",
    "\n",
    "        # words = document.split()\n",
    "\n",
    "        for mapped_word, v in mapping_dict.items():\n",
    "            \n",
    "            document = re.sub(r'\\b{word}\\b'.format(word = v), mapped_word, document)\n",
    "\n",
    "            # for target_word in v:\n",
    "\n",
    "            #     for i, word in enumerate(words):\n",
    "            #         if word == target_word:\n",
    "            #             words[i] = mapped_word\n",
    "\n",
    "        # document = \" \".join(words)\n",
    "        document = document.strip()\n",
    "\n",
    "        processed_corpus.append(document)\n",
    "\n",
    "    return processed_corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_processing(corpus):\n",
    "    '''\n",
    "    Function applies final processing steps post profane mapping such as removing special characters,\n",
    "    punctuations etc.\n",
    "    '''\n",
    "    processed_comments = []\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    print('final_processing')\n",
    "\n",
    "    emoj = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "\n",
    "\n",
    "    # looping through each comment in corpus\n",
    "    for comment in tqdm(corpus):\n",
    "        try:\n",
    "          processed_words = []\n",
    "          comment = re.sub(emoj, r'', comment)\n",
    "          comment = re.sub(r'http\\S+', '', comment)\n",
    "          # comment = re.sub(\"\\B\\#\\w+\", ' ', comment)\n",
    "          # comment = re.sub(\"\\B\\@\\w+\", '', comment)\n",
    "          comment = re.sub(r'(\\w*#\\w+|\\w+#\\w*)','', comment)\n",
    "          comment = re.sub(r'(\\w*@\\w+|\\w+#\\w*)','',  comment)\n",
    "          comment = re.sub(r'[^A-Za-z\\s]+',\"\",  comment) # retain only letters\n",
    "          for word in comment.split():\n",
    "              if len(word) >= 3:\n",
    "                  processed_words.append(word)\n",
    "              \n",
    "          comment = \" \".join([e for e in processed_words if e.lower() is in stopwords_list])\n",
    "          \n",
    "          processed_comments.append(comment.strip())\n",
    "        except Exception as e:\n",
    "          print(corpus)\n",
    "          print(e)\n",
    "          pass\n",
    "    \n",
    "    return processed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanComments(comments):\n",
    "    processed_comments = []\n",
    "    for comment in comments:\n",
    "        comment = decontracted(comment)\n",
    "        comment = removeNonPrintable(comment)\n",
    "\n",
    "        # Lower comment\n",
    "        processed_comments.append(comment.lower().strip())\n",
    "      \n",
    "    profane_words = getProfaneWords()\n",
    "    #unique_words = getUniqueWords(processed_comments)\n",
    "    #profane_dict = createMappingDict(profane_words, unique_words)\n",
    "    #processed_comments = replaceWords(processed_comments, profane_dict)\n",
    "    final_comments = final_processing(processed_comments)\n",
    "    return final_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrHeatmap(df, fileName = None):\n",
    "    classes = df.columns[1:]\n",
    "    data = df.copy()\n",
    "    data = data[classes]\n",
    "    corr = data.corr()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(corr,annot=True,vmin=1,vmax=0,fmt='.2g',cmap='rocket')\n",
    "    plt.title(\"Correlation Matrix: Labels of Comments\")\n",
    "    if fileName:\n",
    "        plt.savefig(fileName + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 43,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPercentiles(data, name):\n",
    "    percentiles_list = []\n",
    "    percentile_names = []\n",
    "    print(\"========== For 0-100 ==========\")\n",
    "    for i in range(10):\n",
    "        print(f'{i*10}th Percentile Value = {np.percentile(data, i*10)}')\n",
    "        percentiles_list.append(np.percentile(data, i*10))\n",
    "        percentile_names.append(i*10)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"========== For 90-100 ==========\")\n",
    "    for i in range(1,11):\n",
    "        print(f'{90+i}th Percentile Value = {np.percentile(data, 90 + i)}')\n",
    "        percentiles_list.append(np.percentile(data, 90+i))\n",
    "        percentile_names.append(90+i)\n",
    "\n",
    "    return pd.DataFrame({'Percentile' : percentile_names, name : percentiles_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHist(data, bin_size, title, fileName = None, columns = None):\n",
    "    if columns is not None:\n",
    "        fig, axis = plt.subplots(len(columns)-1, 1, figsize = (12,30))\n",
    "        for i, col in enumerate(columns[1:]):\n",
    "            sns.histplot(data=data[col],bins=bin_size,palette=\"rocket\",ax = axis[i])\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(top = 0.95)\n",
    "        fig.suptitle(title, size = 18)\n",
    "    else:\n",
    "        sns.histplot(data=data,bins=bin_size,palette=\"rocket\")\n",
    "        plt.title(title)\n",
    "    \n",
    "\n",
    "    if fileName:\n",
    "        plt.savefig(fileName + '.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHist(df_train, 30, 'Histogram of the Labels', 'categories_histogram', df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHist(df_train.Comment.str.len(), 50, 'Length of the Comments', 'comments_length_before_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"============== {'Toxicity'} ==============\")\n",
    "df_percentile = showPercentiles(df_train['Toxicity'], 'Toxicity')\n",
    "print(\"\\n\")\n",
    "for cat in df_train.columns[2:]:\n",
    "    print(f\"============== {cat} ==============\")\n",
    "    pd.concat((df_percentile, showPercentiles(df_train[cat], cat)), axis = 1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPercentiles(df_train.Comment.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrHeatmap(df_train, 'corr_matrix_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a strong positive relationship between Toxicity-Severe Toxicity, Identity Attack-Insult and Toxicity-Profanity respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_comments = cleanComments(df_train.Comment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_train[df_train.columns[1:]].copy()\n",
    "df_processed['ProcessedComment'] = processed_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv('./data/new_train_10k.csv')\n",
    "df_processed.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_processed.columns.tolist()\n",
    "ordered_cols = cols[-1:] + cols[:-1]\n",
    "df_processed = df_processed[ordered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHist(df_processed.ProcessedComment.str.len(), 50, 'Comments Length After Preprocessing', 'after_preprocess_comments_histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the histogram above, the length most of the comments have is in between 0-300. Additionally, the histogram fits to logarithmic distribution and it is right-skewed. To examine the histogram furher, we can look into percentiles of the histogram to determine the vector dimensions which will be crucial for our deep learning model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== For 0-100 ==========\n",
      "0th Percentile Value = 0.0\n",
      "10th Percentile Value = 36.0\n",
      "20th Percentile Value = 62.0\n",
      "30th Percentile Value = 91.0\n",
      "40th Percentile Value = 125.0\n",
      "50th Percentile Value = 167.0\n",
      "60th Percentile Value = 226.0\n",
      "70th Percentile Value = 303.0\n",
      "80th Percentile Value = 439.0\n",
      "90th Percentile Value = 745.0\n",
      "\n",
      "\n",
      "========== For 90-100 ==========\n",
      "91th Percentile Value = 799.0\n",
      "92th Percentile Value = 866.0\n",
      "93th Percentile Value = 932.0\n",
      "94th Percentile Value = 1019.5999999999913\n",
      "95th Percentile Value = 1137.0\n",
      "96th Percentile Value = 1295.0\n",
      "97th Percentile Value = 1505.0\n",
      "98th Percentile Value = 1931.0\n",
      "99th Percentile Value = 2847.0\n",
      "100th Percentile Value = 5000.0\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "df_len = showPercentiles(df_processed.ProcessedComment.str.len(), 'Length of Comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 95% of the comments have 1000 characters approximately. As a result, the input dimension of the tokenized vectors can be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_len.to_csv('comments_length_percentiles.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn dataset into list(list of tokens, scores x6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "# tokenizer = layers.TextVectorization(max_tokens=VOCAB_SIZE, standardize=None, split=\"whitespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_comments = df_processed.dropna().loc[:,\"ProcessedComment\"].convert_dtypes(infer_objects=True).array\n",
    "detections = df_processed.dropna().loc[:,\"Toxicity\": \"Threat\"].convert_dtypes(infer_objects=True).__array__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(processed_comments)\n",
    "tokenized_comments = tokenizer.texts_to_sequences(processed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in tokenized_comments:\n",
    "\ttokens.extend([0] * (VOCAB_SIZE - len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = np.asarray([np.array(input[:VOCAB_SIZE], dtype=int) for input in tokenized_comments[: int(len(tokenized_comments) * .9) ]])\n",
    "train_output = np.asarray(detections[: int(len(tokenized_comments) * .9) ], dtype=np.float32)\n",
    "\n",
    "test_input = np.asarray([np.array(input[:VOCAB_SIZE], dtype=int) for input in tokenized_comments[int(len(tokenized_comments) * .9) :]])\n",
    "test_output = np.asarray(detections[int(len(tokenized_comments) * .9) :], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=64, mask_zero=True))\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(32))\n",
    "model.add(layers.Dense(6))\n",
    "\n",
    "model.compile(loss = keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "\t\t\t  optimizer = keras.optimizers.adam_v2.Adam(1e-4),\n",
    "\t\t\t  metrics = ['Toxicity', 'Severe_Toxicity', 'Identity_Attack', 'Insult', 'Profanity', 'Threat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\losses.py\", line 1930, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\backend.py\", line 5283, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1000, 6) vs (None, 6)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GitHub\\AI\\Toxicity Detection Model\\notebook.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GitHub/AI/Toxicity%20Detection%20Model/notebook.ipynb#ch0000051?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_input,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI/Toxicity%20Detection%20Model/notebook.ipynb#ch0000051?line=1'>2</a>\u001b[0m \t\t\t\t\ty\u001b[39m=\u001b[39;49mtrain_output,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI/Toxicity%20Detection%20Model/notebook.ipynb#ch0000051?line=2'>3</a>\u001b[0m \t\t\t\t\tepochs \u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI/Toxicity%20Detection%20Model/notebook.ipynb#ch0000051?line=3'>4</a>\u001b[0m \t\t\t\t\tsteps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/AI/Toxicity%20Detection%20Model/notebook.ipynb#ch0000051?line=4'>5</a>\u001b[0m \t\t\t\t\tverbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/GitHub/AI/Toxicity%20Detection%20Model/venv/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///d%3A/GitHub/AI/Toxicity%20Detection%20Model/venv/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///d%3A/GitHub/AI/Toxicity%20Detection%20Model/venv/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///d%3A/GitHub/AI/Toxicity%20Detection%20Model/venv/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/GitHub/AI/Toxicity%20Detection%20Model/venv/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filel85ofsia.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ASUS/AppData/Local/Temp/__autograph_generated_filel85ofsia.py?line=12'>13</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/ASUS/AppData/Local/Temp/__autograph_generated_filel85ofsia.py?line=13'>14</a>\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/ASUS/AppData/Local/Temp/__autograph_generated_filel85ofsia.py?line=14'>15</a>\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     <a href='file:///c%3A/Users/ASUS/AppData/Local/Temp/__autograph_generated_filel85ofsia.py?line=15'>16</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/ASUS/AppData/Local/Temp/__autograph_generated_filel85ofsia.py?line=16'>17</a>\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\losses.py\", line 1930, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"d:\\GitHub\\AI\\Toxicity Detection Model\\venv\\lib\\site-packages\\keras\\backend.py\", line 5283, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1000, 6) vs (None, 6)).\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_input,\n",
    "\t\t\t\t\ty=train_output,\n",
    "\t\t\t\t\tepochs = 20,\n",
    "\t\t\t\t\tsteps_per_epoch = 50,\n",
    "\t\t\t\t\tverbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x=test_input, y=test_output)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c27eb6091674c73af7ac585eb40475ca82f7aaec6bf96c7e19d14c7a47e78dc"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 ('python-cvcourse')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
